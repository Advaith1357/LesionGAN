# -*- coding: utf-8 -*-
"""Copy of Copy of Data Augmentation Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z4pZfuSllV8Ho_60p1VEj--yu5Mb3oTt
"""

from google.colab import drive
drive.mount('/content/drive')

train_path = '/content/drive/Shareddrives/1:1 Advaith Menon/Downloads/final_datasets/GAN_Plus_Vanilla90/'
val_path = '/content/drive/Shareddrives/1:1 Advaith Menon/Downloads/final_datasets/Vanilla-NT-Holdout/'

import tensorflow as tf
def create_model(base_model, num_classes):
    x=base_model.output
    x=GlobalAveragePooling2D()(x)
    x=tf.keras.layers.Dense(100,activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(), use_bias=True)(x)
    preds=tf.keras.layers.Dense(num_classes,activation='softmax', kernel_initializer=tf.keras.initializers.VarianceScaling(), use_bias=False)(x)

    model=Model(inputs=base_model.input,outputs=preds)
    return model

from tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, Ftrl, Nadam, RMSprop, SGD
def get_optimizer(optimizer_name, learning_rate):

    print('Selected Optimizer', optimizer_name)
    switcher = {
        'Adadelta': Adadelta(learning_rate=learning_rate),
        'Adagrad': Adagrad(learning_rate=learning_rate),
        'Adam': Adam(learning_rate=learning_rate),
        'Adamax': Adamax(learning_rate=learning_rate),
        'FTRL': Ftrl(learning_rate=learning_rate),
        'NAdam': Nadam(learning_rate=learning_rate),
        'RMSprop': RMSprop(learning_rate=learning_rate),
        'Gradient Descent': SGD(learning_rate=learning_rate)
    }
    return switcher.get(optimizer_name, Adam(learning_rate=learning_rate))

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
from keras.applications.resnet import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense,GlobalAveragePooling2D
from keras.models import Model
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing import image_dataset_from_directory
from keras.callbacks import EarlyStopping
from tensorflow import keras

epochs = 15
base_learning_rate = 0.0001
optimizer = 'Adam'
BATCH_SIZE = 32
num_classes = 8
IMG_SIZE = (224, 224)


train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0)
train_generator = train_datagen.flow_from_directory(train_path,
                                                target_size=IMG_SIZE,
                                                color_mode='rgb',
                                                batch_size=BATCH_SIZE,
                                                class_mode='categorical',
                                                shuffle=True,
                                                subset = 'training')
validation_generator = train_datagen.flow_from_directory(val_path,
                                                target_size=IMG_SIZE,
                                                color_mode='rgb',
                                                batch_size=BATCH_SIZE,
                                                class_mode='categorical',
                                                shuffle=False,
                                                subset = 'training')


base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet', alpha=0.35)
for layer in base_model.layers:
    layer.trainable=False
model = create_model(base_model,num_classes)
model.compile(optimizer = get_optimizer(optimizer_name=optimizer,learning_rate=base_learning_rate),loss='CategoricalCrossentropy',metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
early_stopping_monitor = EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=30,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=True
)
step_size_train = train_generator.n//train_generator.batch_size
history_fine = model.fit(train_generator,
                        epochs=epochs,
                        callbacks=[early_stopping_monitor],
                        validation_data = validation_generator,
                        verbose=1)

# Import numpy for calculating best model accuracy
import numpy as np
prc = history_fine.history['val_precision_2']
rec = history_fine.history['val_recall_2']
acc = history_fine.history['val_accuracy']
loss = history_fine.history['val_loss']

prc

def seperate_labels(generator):
    x_validation = []
    y_validation = []
    num_seen = 0

    for x, labels in generator:
        x_validation.append(x)
        y_validation.append([argmax(label) for label in labels])
        num_seen += len(x)
        if num_seen == generator.n: break

    x_validation = np.concatenate(x_validation)
    y_validation = np.concatenate(y_validation)
    return x_validation, y_validation

# Calculate and display the confusion matrix
import matplotlib.pyplot as plt
from numpy.core.fromnumeric import argmax
from sklearn.metrics import ConfusionMatrixDisplay

x_validation, y_validation = seperate_labels(validation_generator)
y_pred = model.predict(x_validation, batch_size=BATCH_SIZE)
predictions = np.apply_along_axis(argmax, 1, y_pred)
display_labels = validation_generator.class_indices.keys()

ConfusionMatrixDisplay.from_predictions(y_validation, predictions, display_labels=display_labels, cmap="binary")
plt.show()

accuracy_with_original_dataset =acc
loss_with_original_dataset = loss
precision_with_original_dataset =prc
recall_with_original_dataset = rec
print(len(accuracy_with_original_dataset))

accuracy_with_augmented_dataset =acc
loss_with_augmented_dataset = loss
precision_with_augmented_dataset =prc
recall_with_augmented_dataset = rec

accuracy_with_GAN_augmented_dataset = acc
loss_with_GAN_augmented_dataset =  loss
precision_with_GAN_augmented_dataset =  prc
recall_with_GAN_augmented_dataset =  rec

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
plt.plot(np.arange(15), accuracy_with_original_dataset, color='green',
     linewidth=2, markersize=12)

plt.xlabel("Epoch Count")
plt.ylabel("Accuracy")

plt.show()

plt.plot(np.arange(15), accuracy_with_augmented_dataset, color='red',
     linewidth=2, markersize=12)

plt.xlabel("Epoch Count")
plt.ylabel("Accuracy")

plt.show()

plt.plot(np.arange(15), accuracy_with_GAN_augmented_dataset, color='blue',
     linewidth=2, markersize=12)

plt.xlabel("Epoch Count")
plt.ylabel("Accuracy")

plt.show()

plt.plot(np.arange(15), loss_with_original_dataset, color='green',
     linewidth=2, markersize=12)

plt.xlabel("Epoch Count")
plt.ylabel("Loss")

plt.show()

plt.plot(np.arange(15), loss_with_augmented_dataset, color='red',
     linewidth=2, markersize=12)

plt.xlabel("Epoch Count")
plt.ylabel("Loss")

plt.show()

plt.plot(np.arange(15), loss_with_GAN_augmented_dataset, color='blue',
     linewidth=2, markersize=12)

plt.xlabel("Epoch Count")
plt.ylabel("Loss")

plt.show()

plt.plot(recall_with_original_dataset, precision_with_original_dataset, color='green',
     linewidth=2, markersize=12)
plt.xlabel("Recall")
plt.ylabel("Precision")

plt.show()

plt.plot(recall_with_augmented_dataset, precision_with_augmented_dataset, color='red',
     linewidth=2, markersize=12)
plt.xlabel("Recall")
plt.ylabel("Precision")

plt.show()

plt.plot(recall_with_GAN_augmented_dataset, precision_with_GAN_augmented_dataset, color='blue',
     linewidth=2, markersize=12)
plt.xlabel("Recall")
plt.ylabel("Precision")

plt.show()